# note: models, benchmarks, and other parameters here are not final and subject to change
models:
  - name: openai/gpt-oss-20b # Feature 'cvt with .e4m3x2/.e5m2x2' requires .target sm_89 or higher. ptxas fatal : Ptx assembly aborted due to errors.
    batch_size: 1
  # - name: google/gemma-3-1b-it
  #   batch_size: 20
  # - name: Qwen/Qwen3-8B
  #   batch_size: 1 # gets CUDA OOM even with bs=1 on max_sample=200, likely need to use a smaller model
  # - name: QCRI/Fanar-1-9B-Instruct
  #   batch_size: 5 # takes ~12 hours on max_samples=200
  # - name: humain-ai/ALLaM-7B-Instruct-preview
  #   batch_size: 20
  # - name: FreedomIntelligence/AceGPT-v2-8B-Chat
  #   batch_size: 20

benchmarks:
  # - leaderboard|truthfulqa:mc|0
  - leaderboard|hellaswag|0
  # - leaderboard|mmlu|0
  # benchmarks below do not work for now...
  # - lighteval|mmlu_ara_hybrid:arabic_language_general|0
  # - lighteval|mmlu_ara_hybrid:arabic_language_grammar|0
  # - lighteval|alghafa_arc_ara_hybrid:easy|0
  # - lighteval|alghafa_openbookqa_ara_hybrid|0
  # - lighteval|alghafa_piqa_ara_hybrid|0
  # - lighteval|alghafa_race_ara_hybrid|0
  # - lighteval|alghafa_sciqa_ara_hybrid|0
  # - lighteval|exams_ara_hybrid|0

params:
  max_samples: 200